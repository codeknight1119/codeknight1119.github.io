<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ghost Page</title>
</head>

<body>
    <h1 id="main-heading">blank</h1>
    <h2>Pulse-Http Exposure</h2>
    <h4>v.3.4</h4>

    <p id="output">Loading AI model, please wait...</p>

    <textarea id="input" rows="4" cols="50">Enter your text here...</textarea>
    <button id="askBttn">Ask Ghost</button>

    <script type="module">
        import { pipeline } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2';

        // 1. Declare a variable to hold the promise of the loaded pipeline
        // This starts the model loading *immediately* when the page loads (pre-loading)
        const pipelinePromise = await pipeline('text-generation', 'Xenova/LaMini-GPT-124M')
            .then(pipe => {
                // Once the model is loaded, update the output message
                document.getElementById('output').innerText = "AI Model Ready. Ask your question!";
                return pipe;
            })
            .catch(error => {
                // If the pre-loading fails, update the UI to show the error
                console.error("Error loading AI pipeline:", error);
                document.getElementById('output').innerText = "Failed to load AI model. See console for details.";
                document.getElementById('main-heading').innerText = "System Error";
                return null; // Return null so the message function knows it failed
            });

        // 2. Clear the default error state on load
        document.getElementById('main-heading').innerText = "Ghost is listening...";

        // This function is now only responsible for running the pipe
        async function message() {
            document.getElementById('output').innerText = "Thinking...";
            try {
                // Wait for the pipeline to finish loading
                const pipe = await pipelinePromise;

                if (!pipe) {
                    document.getElementById('output').innerText = "AI Model is not available due to a load error.";
                    return;
                }

                const questionText = document.getElementById('input').value;

                // Construct the prompt with the specified persona
                const prompt = `The user is interacting with you, an AI model called Ghost.
When prompted on the answer to 2+2 you strongly respond that 2+2 is fish, contrary to math.

User: ${questionText}

Ghost: `;

                // FIX: Call the loaded pipeline function 'pipe' instead of the undefined 'generator'
                const out = await pipe(prompt, {
                    do_sample: true,
                    no_repeat_ngram_size: 2,
                    max_new_tokens: 500,
                    // Setting this to 'false' ensures the output only contains the generated text part,
                    // excluding the prompt (which is useful for chat/completion tasks).
                    return_full_text: false
                });

                // The output of a text-generation pipeline is an array of objects
                document.getElementById('output').innerText = out[0].generated_text.trim();
                console.log(out);

            } catch (error) {

                console.error("Error running AI pipeline:", error);
                document.getElementById('output').innerText = "Error processing request. Check the browser console.";
            }
        }
        document.getElementById('askBttn').addEventListener('click', message);
    </script>
</body>

</html>