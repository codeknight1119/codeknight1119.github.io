<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <title>Unit 6 AI RAG</title>
    <style>
        /* Minimal styling */
        html, body { height: 100%; margin: 0; font-family: system-ui, sans-serif; background: #fff; color: #111; }
        .container { display: flex; flex-direction: column; align-items: center; justify-content: flex-start; padding: 2rem; max-width: 800px; margin: 0 auto; }
        h1 { margin-bottom: 1rem; }
        
        #chat-history {
            width: 100%;
            height: 300px;
            border: 1px solid #ccc;
            overflow-y: auto;
            margin-bottom: 1rem;
            padding: 1rem;
            background: #f9f9f9;
            border-radius: 8px;
        }
        
        .input-group { display: flex; width: 100%; gap: 10px; }
        input { flex-grow: 1; padding: 10px; border: 1px solid #ccc; border-radius: 4px; }
        button { padding: 10px 20px; cursor: pointer; background: #007bff; color: white; border: none; border-radius: 4px; }
        button:disabled { background: #ccc; }
        
        .message { margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #eee; }
        .user-msg { font-weight: bold; color: #0056b3; }
        .ai-msg { color: #333; }
        
        #status { font-size: 0.9rem; color: #666; margin-bottom: 1rem; font-style: italic; }
    </style>
</head>

<body>
    <div class="container">
        <h1>Unit 6 AI</h1>
        
        <div id="chat-history"></div>

        <div class="input-group">
            <input type="text" id="userInput" placeholder="Ask a question.">
            <button id="sendButton">Send</button>
        </div>
    </div>

    <script type="module">
   import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.2';

        // Configuration: Skip local model checks since we are fetching from CDN
        env.allowLocalModels = false;

        // DOM Elements
        const chatHistory = document.getElementById('chat-history');
        const userInput = document.getElementById('userInput');
        const sendButton = document.getElementById('sendButton');
        const statusDiv = document.createElement('div');
        statusDiv.id = 'status';
        document.querySelector('.container').insertBefore(statusDiv, chatHistory);

        let contextChunks = [];
        let contextEmbeddings = [];
        let embedder_model = null;
        let generator_model = null;

        // --- 1. Initialization Function ---
        async function init() {
            updateStatus("Downloading chunks file...");
            
            try {
                // FETCH: Get the raw text file
                // Ensure this URL is accessible (CORS headers enabled if cross-domain)
                const response = await fetch("https://codeknight1119.github.io/aiForUnit6/chunks.txt"); 
                if (!response.ok) throw new Error("Failed to load chunks.txt");
                const text = await response.text();

                // PARSE: Split text into array (Assuming newlines separate chunks)
                // Filter removes empty lines
                contextChunks = text.split('\n').filter(line => line.trim().length > 0);
                
                updateStatus(`Loaded ${contextChunks.length} chunks. Loading embedding model...`);

                // LOAD MODELS
                embedder_model = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2');
                
                updateStatus("Embedding chunks (this may take a moment)...");
                
                // EMBED: Create vectors for every chunk
                // We map over chunks and embed them one by one
                for (const chunk of contextChunks) {
                    const output = await embedder_model(chunk, { pooling: 'mean', normalize: true });
                    contextEmbeddings.push(output.data);
                }

                updateStatus("Loading generation model (LaMini-Flan-T5)...");
                generator_model = await pipeline('text2text-generation', 'Xenova/LaMini-Flan-T5-783M');

                updateStatus("Ready! Ask a question.");
                sendButton.disabled = false;

            } catch (err) {
                updateStatus("Error: " + err.message);
                console.error(err);
            }
        }

        // --- 2. Helper: Cosine Similarity ---
        function cosineSimilarity(vecA, vecB) {
            let dotProduct = 0;
            let magnitudeA = 0;
            let magnitudeB = 0;
            for (let i = 0; i < vecA.length; i++) {
                dotProduct += vecA[i] * vecB[i];
                magnitudeA += vecA[i] * vecA[i];
                magnitudeB += vecB[i] * vecB[i];
            }
            return dotProduct / (Math.sqrt(magnitudeA) * Math.sqrt(magnitudeB));
        }

        // --- 3. Helper: Retrieve Best Context ---
        async function retrieveContext(query) {
            // Embed the user's query
            const queryOutput = await embedder_model(query, { pooling: 'mean', normalize: true });
            const queryEmbedding = queryOutput.data;

            // Compare query vs all chunk embeddings
            const scores = contextEmbeddings.map((embedding, index) => ({
                score: cosineSimilarity(queryEmbedding, embedding),
                text: contextChunks[index]
            }));

            // Sort by highest score and take top 1 or 2
            scores.sort((a, b) => b.score - a.score);
            return scores[0].text; // Returning the single best match
        }

        // --- 4. Chat Interaction Logic ---
        async function handleChat() {
            const question = userInput.value.trim();
            if (!question) return;

            // UI Updates
            appendMessage("User", question);
            userInput.value = '';
            userInput.disabled = true;
            sendButton.disabled = true;
            updateStatus("Thinking...");

            try {
                // RAG Step 1: Retrieve
                const context = await retrieveContext(question);
                console.log("Retrieved Context:", context); // For debugging

                // RAG Step 2: Generate
                // Construct a prompt that forces the AI to use the context
                const prompt = `Context: ${context}\n\nQuestion: ${question}\n\nAnswer:`;

                const result = await generator_model(prompt, {
                    max_new_tokens: 128,
                    temperature: 0.7,
                    repetition_penalty: 1.2
                });

                appendMessage("AI", result[0].generated_text);
            } catch (err) {
                appendMessage("System", "Error processing request.");
                console.error(err);
            }

            userInput.disabled = false;
            sendButton.disabled = false;
            updateStatus("Ready.");
        }

        function appendMessage(sender, text) {
            const msgDiv = document.createElement('div');
            msgDiv.className = 'message';
            msgDiv.innerHTML = `<span class="${sender === 'User' ? 'user-msg' : 'ai-msg'}">${sender}:</span> ${text}`;
            chatHistory.appendChild(msgDiv);
            chatHistory.scrollTop = chatHistory.scrollHeight;
        }

        function updateStatus(text) {
            statusDiv.innerText = text;
        }

        // Event Listeners
        sendButton.addEventListener('click', handleChat);
        userInput.addEventListener('keypress', (e) => {
            if (e.key === 'Enter') handleChat();
        });

        // Start
        sendButton.disabled = true; // Disable until loaded
        init();
    </script>
</body>
</html>