<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <title>Unit 6 AI RAG</title>
    <style>
        /* Minimal styling */
        html, body { height: 100%; margin: 0; font-family: system-ui, sans-serif; background: #fff; color: #111; }
        .container { display: flex; flex-direction: column; align-items: center; justify-content: flex-start; padding: 2rem; max-width: 800px; margin: 0 auto; }
        h1 { margin-bottom: 1rem; }
        
        #chat-history {
            width: 100%;
            height: 300px;
            border: 1px solid #ccc;
            overflow-y: auto;
            margin-bottom: 1rem;
            padding: 1rem;
            background: #f9f9f9;
            border-radius: 8px;
        }
        
        .input-group { display: flex; width: 100%; gap: 10px; }
        input { flex-grow: 1; padding: 10px; border: 1px solid #ccc; border-radius: 4px; }
        button { padding: 10px 20px; cursor: pointer; background: #007bff; color: white; border: none; border-radius: 4px; }
        button:disabled { background: #ccc; }
        
        .message { margin-bottom: 1rem; padding-bottom: 1rem; border-bottom: 1px solid #eee; }
        .user-msg { font-weight: bold; color: #0056b3; }
        .ai-msg { color: #333; }
        
        #status { font-size: 0.9rem; color: #666; margin-bottom: 1rem; font-style: italic; }
    </style>
</head>

<body>
    <div class="container">
        <h1>Unit 6 AI Tutor</h1>
        
        <div id="status">Initializing... Please wait for models to load.</div>

        <div id="chat-history"></div>

        <div class="input-group">
            <input type="text" id="userInput" placeholder="Ask a question about the slides..." disabled>
            <button id="sendButton" disabled>Send</button>
        </div>
    </div>

    <script type="module">
        // Import Transformers.js from CDN
        import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.2';

        // Skip local model checks since we are in browser
        env.allowLocalModels = false;

        // Variables to hold our models and data
        let embedder = null;
        let generator = null;
        let chunks = [];
        let chunkEmbeddings = [];

        const statusEl = document.getElementById('status');
        const inputEl = document.getElementById('userInput');
        const btnEl = document.getElementById('sendButton');
        const chatHistory = document.getElementById('chat-history');

        // --- 1. SETUP: Load Data & Models ---
        async function initialize() {
            try {
                // A. Fetch the chunks file
                statusEl.innerText = "üì• Loading slide content...";
                const response = await fetch("aiForUnit6/chunks.txt");
                if (!response.ok) throw new Error("Could not find chunks.txt");
                const text = await response.text();
                
                // Split text into array. Assumes chunks are separated by double newlines or a delimiter.
                // Adjust '\n\n' to match how you saved your chunks.
                chunks = text.split('\n\n').filter(c => c.trim().length > 0);
                console.log(`Loaded ${chunks.length} chunks.`);

                // B. Load Embedding Model (Same as your Python script: all-MiniLM-L6-v2)
                statusEl.innerText = "üß† Loading embedding model (20MB)...";
                embedder = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2');

                // C. Pre-compute embeddings for all chunks (The Indexing Step)
                statusEl.innerText = "‚öôÔ∏è Indexing slides...";
                for (const chunk of chunks) {
                    const output = await embedder(chunk, { pooling: 'mean', normalize: true });
                    chunkEmbeddings.push(output.data);
                }

                // D. Load Generation Model (Using LaMini-Flan-T5 because 'large' is too big for browsers)
                // Note: 'google/flan-t5-large' is ~3GB. 'LaMini-Flan-T5-248M' is ~250MB and runs fast in browser.
                statusEl.innerText = "ü§ñ Loading text generator (250MB - this takes a moment)...";
                generator = await pipeline('text2text-generation', 'Xenova/LaMini-Flan-T5-248M');

                // Ready!
                statusEl.innerText = "‚úÖ Ready! Ask me anything about Unit 6.";
                inputEl.disabled = false;
                btnEl.disabled = false;

            } catch (err) {
                statusEl.innerText = "‚ùå Error: " + err.message;
                console.error(err);
            }
        }

        // --- 2. LOGIC: Math for Cosine Similarity ---
        // Since we don't have FAISS, we do the math manually.
        function cosineSimilarity(vecA, vecB) {
            let dotProduct = 0;
            let magnitudeA = 0;
            let magnitudeB = 0;
            for (let i = 0; i < vecA.length; i++) {
                dotProduct += vecA[i] * vecB[i];
                magnitudeA += vecA[i] * vecA[i];
                magnitudeB += vecB[i] * vecB[i];
            }
            return dotProduct / (Math.sqrt(magnitudeA) * Math.sqrt(magnitudeB));
        }

        async function retrieveContext(query) {
            // Embed the user query
            const queryOutput = await embedder(query, { pooling: 'mean', normalize: true });
            const queryEmbedding = queryOutput.data;

            // Calculate similarity score for every chunk
            const scoredChunks = chunks.map((chunk, index) => {
                return {
                    text: chunk,
                    score: cosineSimilarity(queryEmbedding, chunkEmbeddings[index])
                };
            });

            // Sort by highest score and take top 3
            scoredChunks.sort((a, b) => b.score - a.score);
            return scoredChunks.slice(0, 3).map(item => item.text);
        }

        // --- 3. UI: Handle Interaction ---
        btnEl.addEventListener('click', async () => {
            const query = inputEl.value.trim();
            if (!query) return;

            // Display User Message
            chatHistory.innerHTML += `<div class="message"><div class="user-msg">You: ${query}</div></div>`;
            inputEl.value = '';
            statusEl.innerText = "ü§î Thinking...";

            // 1. Retrieve
            const contextChunks = await retrieveContext(query);
            const context = contextChunks.join("\n\n");

            // 2. Prompt
            const prompt = `Instruction: Answer the question based on the context below. Keep it concise.\n\nContext:\n${context}\n\nQuestion: ${query}\n\nAnswer:`;

            // 3. Generate
            try {
                const output = await generator(prompt, {
                    max_new_tokens: 150,
                    temperature: 0.5,
                    repetition_penalty: 1.2
                });

                const answer = output[0].generated_text;

                // Display AI Message
                chatHistory.innerHTML += `<div class="message"><div class="ai-msg"><strong>AI:</strong> ${answer}</div></div>`;
                chatHistory.scrollTop = chatHistory.scrollHeight; // Auto-scroll to bottom
                statusEl.innerText = "‚úÖ Ready";
            } catch (err) {
                console.error(err);
                statusEl.innerText = "‚ùå Error generating answer.";
            }
        });

        // Start the app
        initialize();
    </script>
</body>
</html>